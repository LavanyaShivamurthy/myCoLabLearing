import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support
from sklearn.multioutput import MultiOutputClassifier
from xgboost import XGBClassifier
from imblearn.over_sampling import SMOTE, ADASYN
from imblearn.combine import SMOTETomek
import warnings
warnings.filterwarnings('ignore')

def load_and_preprocess_data(data_path):
    """
    Load the hospital network data and perform initial preprocessing
    """
    # Load the data
    df = pd.read_csv(data_path)
    
    # Display basic information
    print(f"Dataset shape: {df.shape}")
    
    # Check for missing values
    missing_values = df.isnull().sum()
    print(f"Missing values per column:\n{missing_values[missing_values > 0]}")
    
    # Drop rows with missing values or impute them based on your strategy
    df = df.dropna()
    
    return df

def extract_features(df):
    """
    Extract and engineer features from network data
    """
    # Create features specific to hospital network traffic
    
    # Time-based features
    if 'frame.time_delta' in df.columns:
        df['time_delta_ms'] = df['frame.time_delta'].astype(float) * 1000  # Convert to milliseconds
    if 'frame.len' in df.columns:
        df['packet_size'] = df['frame.len'].astype(float)
    
    # TCP flag-based features
    flag_columns = [col for col in df.columns if 'tcp.flags' in col]
    for col in flag_columns:
        df[col] = df[col].fillna(0).astype(int)
    
    # Create aggregated features
    if 'tcp.len' in df.columns and 'frame.len' in df.columns:
        df['tcp_payload_ratio'] = df['tcp.len'] / df['frame.len']
    
    # TTL-based features - can help identify certain types of critical traffic
    if 'ip.ttl' in df.columns:
        df['ttl'] = df['ip.ttl'].astype(float)
    
    # Protocol features
    if 'ip.proto' in df.columns:
        df['protocol'] = df['ip.proto'].astype(float)
    
    # MQTT specific features (if available)
    mqtt_columns = [col for col in df.columns if 'mqtt' in col]
    if mqtt_columns:
        # Convert categorical MQTT fields to numeric
        for col in mqtt_columns:
            if col in df.columns and df[col].dtype == 'object':
                df[col] = pd.factorize(df[col])[0]
    
    # Source/destination features
    if 'ip.src' in df.columns and 'ip.dst' in df.columns:
        # Create categorical encodings for source and destination IPs
        df['src_ip_encoded'] = pd.factorize(df['ip.src'])[0]
        df['dst_ip_encoded'] = pd.factorize(df['ip.dst'])[0]
    
    # Port-based features
    if 'tcp.srcport' in df.columns and 'tcp.dstport' in df.columns:
        df['srcport'] = df['tcp.srcport'].astype(float)
        df['dstport'] = df['tcp.dstport'].astype(float)
        
        # Create features for well-known medical ports
        # Example medical ports (replace with actual medical device ports used in your hospital)
        medical_ports = [80, 443, 8080, 2575, 2576, 104, 11112]  # Common DICOM, HL7, web ports
        emergency_ports = [2575, 11112, 104]  # Critical for DICOM/HL7 in emergency situations
        
        df['is_medical_src_port'] = df['tcp.srcport'].isin(medical_ports).astype(int)
        df['is_medical_dst_port'] = df['tcp.dstport'].isin(medical_ports).astype(int)
        df['is_emergency_port'] = (df['tcp.srcport'].isin(emergency_ports) | 
                                  df['tcp.dstport'].isin(emergency_ports)).astype(int)
    
    # Additional TCP features
    tcp_features = ['tcp.len', 'tcp.ack', 'tcp.flags.ack', 'tcp.flags.push', 
                   'tcp.flags.syn', 'tcp.window_size_value']
    tcp_numeric_features = []
    
    for feature in tcp_features:
        if feature in df.columns:
            feature_name = feature.replace('.', '_')
            df[feature_name] = df[feature].fillna(0).astype(float)
            tcp_numeric_features.append(feature_name)
    
    # Packet timing features (can help identify emergency traffic patterns)
    if 'tcp.time_delta' in df.columns:
        df['tcp_time_delta'] = df['tcp.time_delta'].fillna(0).astype(float)
        
        # Calculate statistics in a rolling window
        df['tcp_time_delta_rolling_mean'] = df['tcp_time_delta'].rolling(window=5, min_periods=1).mean()
        df['tcp_time_delta_rolling_std'] = df['tcp_time_delta'].rolling(window=5, min_periods=1).std().fillna(0)
    
    # Select numerical features for the model
    numerical_features = []
    
    # Add basic features if they exist
    basic_features = ['time_delta_ms', 'packet_size', 'tcp_payload_ratio', 
                      'src_ip_encoded', 'dst_ip_encoded', 'ttl', 'protocol',
                      'srcport', 'dstport', 'is_medical_src_port', 'is_medical_dst_port',
                      'is_emergency_port', 'tcp_time_delta', 'tcp_time_delta_rolling_mean',
                      'tcp_time_delta_rolling_std']
    
    for feature in basic_features:
        if feature in df.columns:
            numerical_features.append(feature)
    
    # Add TCP flag features
    numerical_features.extend([col for col in flag_columns if col in df.columns])
    
    # Add TCP numeric features
    numerical_features.extend(tcp_numeric_features)
    
    # Add MQTT features if available
    numerical_features.extend([col for col in mqtt_columns if col in df.columns])
    
    # Filter to include only available columns
    numerical_features = [col for col in numerical_features if col in df.columns]
    
    print(f"Selected features: {numerical_features}")
    print(f"Number of features: {len(numerical_features)}")
    
    return df, numerical_features

def create_enhanced_target_variables(df):
    """
    Create more sophisticated target variables using domain-specific rules for hospital networks
    """
    # Initialize target variables
    df['emergency'] = 0
    df['important'] = 0
    
    # Rule-based classification for hospital network traffic
    # These rules should be adapted to your specific hospital network configuration
    
    # === EMERGENCY RULES ===
    
    # Rule 1: Traffic on known emergency ports
    if 'tcp.dstport' in df.columns:
        emergency_ports = [2575, 11112, 104, 2762]  # DICOM, HL7, emergency system ports
        df.loc[df['tcp.dstport'].isin(emergency_ports), 'emergency'] = 1
        df.loc[df['tcp.srcport'].isin(emergency_ports), 'emergency'] = 1
    
    # Rule 2: Traffic with urgent TCP flag
    if 'tcp.flags.urg' in df.columns:
        df.loc[df['tcp.flags.urg'] == 1, 'emergency'] = 1
    
    # Rule 3: Short time deltas (bursts of traffic) could indicate emergency situations
    if 'tcp.time_delta' in df.columns:
        # Very short time deltas might indicate emergency traffic
        df.loc[df['tcp.time_delta'] < 0.01, 'emergency'] = 1
    
    # Rule 4: Traffic to/from specific emergency department systems
    if 'ip.dst' in df.columns:
        # Replace with actual emergency department IP addresses
        emergency_ips = ['192.168.1.100', '10.0.0.50']  
        df.loc[df['ip.dst'].isin(emergency_ips), 'emergency'] = 1
        
    # Rule 5: Traffic with specific characteristics of medical emergencies
    # For example, large MQTT messages with emergency-related topics
    if 'mqtt.topic' in df.columns:
        emergency_keywords = ['emergency', 'alert', 'critical', 'urgent']
        for keyword in emergency_keywords:
            df.loc[df['mqtt.topic'].astype(str).str.contains(keyword, case=False, na=False), 'emergency'] = 1
    
    # === IMPORTANCE RULES ===
    
    # Rule 1: Traffic on ports used by important hospital systems
    if 'tcp.dstport' in df.columns:
        important_ports = [80, 443, 8080, 3306, 1521, 2575, 2576, 104, 11112]  # Web, DB, DICOM, HL7
        df.loc[df['tcp.dstport'].isin(important_ports), 'important'] = 1
    
    # Rule 2: Traffic with larger packets (might contain important medical data)
    if 'frame.len' in df.columns:
        df.loc[df['frame.len'] > 1500, 'important'] = 1
    
    # Rule 3: Traffic with push flag set (indicates important data being sent)
    if 'tcp.flags.push' in df.columns:
        df.loc[df['tcp.flags.push'] == 1, 'important'] = 1
    
    # Rule 4: Traffic to/from important hospital systems
    if 'ip.dst' in df.columns:
        # Replace with actual important system IP addresses
        important_ips = ['192.168.1.10', '10.0.0.20']  
        df.loc[df['ip.dst'].isin(important_ips), 'important'] = 1
    
    # Rule 5: MQTT traffic with important topics
    if 'mqtt.topic' in df.columns:
        important_keywords = ['patient', 'vitals', 'monitor', 'record', 'admin']
        for keyword in important_keywords:
            df.loc[df['mqtt.topic'].astype(str).str.contains(keyword, case=False, na=False), 'important'] = 1
    
    # === ADDITIONAL ARTIFICIAL RULES TO CREATE BALANCED CLASSES ===
    # These rules are designed to create more balanced classes for better model training
    
    # Create some emergency packets based on specific packet size ranges
    if 'frame.len' in df.columns:
        # Assign some packets in specific size ranges as emergency
        size_ranges = [(200, 300), (500, 600), (900, 1000)]
        
        for lower, upper in size_ranges:
            # Select a small percentage of packets in this range
            mask = (df['frame.len'] >= lower) & (df['frame.len'] <= upper)
            sample_size = min(int(mask.sum() * 0.05), 1000)  # Take 5% or max 1000
            
            if sample_size > 0:
                # Get indices of packets in this range
                indices = df.loc[mask].index
                
                # Sample some of them and mark as emergency
                if len(indices) > 0:
                    sampled_indices = np.random.choice(indices, size=min(sample_size, len(indices)), replace=False)
                    df.loc[sampled_indices, 'emergency'] = 1
    
    # Create some important packets based on TTL values
    if 'ip.ttl' in df.columns:
        # Assign some packets with specific TTL values as important
        ttl_values = [64, 128, 255]
        
        for ttl in ttl_values:
            # Select a small percentage of packets with this TTL
            mask = df['ip.ttl'] == ttl
            sample_size = min(int(mask.sum() * 0.05), 1000)  # Take 5% or max 1000
            
            if sample_size > 0:
                # Get indices of packets with this TTL
                indices = df.loc[mask].index
                
                # Sample some of them and mark as important
                if len(indices) > 0:
                    sampled_indices = np.random.choice(indices, size=min(sample_size, len(indices)), replace=False)
                    df.loc[sampled_indices, 'important'] = 1
    
    # Count instances of each class
    emergency_count = df['emergency'].sum()
    important_count = df['important'].sum()
    
    print(f"Emergency packets: {emergency_count} ({emergency_count/len(df)*100:.2f}%)")
    print(f"Important packets: {important_count} ({important_count/len(df)*100:.2f}%)")
    
    # Create the 4-class priority label
    df['priority_class'] = 4  # Default: Not Emergency and Not Important (lowest priority)
    df.loc[(df['emergency'] == 1) & (df['important'] == 1), 'priority_class'] = 1  # Emergency and Important
    df.loc[(df['emergency'] == 1) & (df['important'] == 0), 'priority_class'] = 2  # Emergency but Not Important
    df.loc[(df['emergency'] == 0) & (df['important'] == 1), 'priority_class'] = 3  # Not Emergency but Important
    
    # Count instances of each priority class
    for i in range(1, 5):
        count = (df['priority_class'] == i).sum()
        print(f"Priority class {i}: {count} ({count/len(df)*100:.2f}%)")
    
    return df

def train_imbalanced_multitask_model(X, y):
    """
    Train a multitask classification model with handling for imbalanced classes
    """
    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    
    print(f"Training set size: {X_train.shape}")
    print(f"Testing set size: {X_test.shape}")
    
    # Handle class imbalance using SMOTE
    print("Applying SMOTE to balance classes...")
    smote = SMOTETomek(random_state=42)
    
    # Apply SMOTE for each task separately
    emergency_X_train, emergency_y_train = smote.fit_resample(X_train, y_train['emergency'])
    print(f"After SMOTE, emergency class distribution: {np.bincount(emergency_y_train)}")
    
    important_X_train, important_y_train = smote.fit_resample(X_train, y_train['important'])
    print(f"After SMOTE, important class distribution: {np.bincount(important_y_train)}")
    
    # Create preprocessing pipeline
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', StandardScaler(), list(range(X.shape[1])))
        ])
    
    # Train separate models for each task
    # Emergency prediction model
    emergency_model = Pipeline([
        ('preprocessor', preprocessor),
        ('classifier', XGBClassifier(
            n_estimators=200,
            learning_rate=0.1,
            max_depth=5,
            subsample=0.8,
            colsample_bytree=0.8,
            scale_pos_weight=10,  # Additional weight for positive class
            random_state=42
        ))
    ])
    
    # Important prediction model
    important_model = Pipeline([
        ('preprocessor', preprocessor),
        ('classifier', XGBClassifier(
            n_estimators=200,
            learning_rate=0.1,
            max_depth=5,
            subsample=0.8,
            colsample_bytree=0.8,
            scale_pos_weight=10,  # Additional weight for positive class
            random_state=42
        ))
    ])
    
    print("Training emergency model...")
    emergency_model.fit(emergency_X_train, emergency_y_train)
    
    print("Training importance model...")
    important_model.fit(important_X_train, important_y_train)
    
    # Return both models and test data
    return emergency_model, important_model, X_test, y_test

def evaluate_imbalanced_models(emergency_model, important_model, X_test, y_test):
    """
    Evaluate models trained on imbalanced data with appropriate metrics
    """
    # Predict with each model
    emergency_pred = emergency_model.predict(X_test)
    important_pred = important_model.predict(X_test)
    
    # Evaluate emergency prediction
    print("\nEvaluation for emergency prediction:")
    print(classification_report(y_test['emergency'], emergency_pred))
    
    # Special metrics for imbalanced data
    precision, recall, f1, _ = precision_recall_fscore_support(y_test['emergency'], emergency_pred, average='binary')
    print(f"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}")
    
    # Create confusion matrix
    cm = confusion_matrix(y_test['emergency'], emergency_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Emergency', 'Emergency'], 
               yticklabels=['Not Emergency', 'Emergency'])
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title('Confusion Matrix for Emergency Prediction')
    plt.show()
    
    # Evaluate importance prediction
    print("\nEvaluation for importance prediction:")
    print(classification_report(y_test['important'], important_pred))
    
    # Special metrics for imbalanced data
    precision, recall, f1, _ = precision_recall_fscore_support(y_test['important'], important_pred, average='binary')
    print(f"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}")
    
    # Create confusion matrix
    cm = confusion_matrix(y_test['important'], important_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Important', 'Important'], 
               yticklabels=['Not Important', 'Important'])
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title('Confusion Matrix for Importance Prediction')
    plt.show()
    
    # Calculate the 4-class priority
    y_test_combined = y_test.copy()
    y_test_combined['pred_emergency'] = emergency_pred
    y_test_combined['pred_important'] = important_pred
    
    y_test_combined['true_priority'] = 4  # Default: Not Emergency and Not Important
    y_test_combined.loc[(y_test_combined['emergency'] == 1) & (y_test_combined['important'] == 1), 'true_priority'] = 1
    y_test_combined.loc[(y_test_combined['emergency'] == 1) & (y_test_combined['important'] == 0), 'true_priority'] = 2
    y_test_combined.loc[(y_test_combined['emergency'] == 0) & (y_test_combined['important'] == 1), 'true_priority'] = 3
    
    y_test_combined['pred_priority'] = 4  # Default: Not Emergency and Not Important
    y_test_combined.loc[(y_test_combined['pred_emergency'] == 1) & (y_test_combined['pred_important'] == 1), 'pred_priority'] = 1
    y_test_combined.loc[(y_test_combined['pred_emergency'] == 1) & (y_test_combined['pred_important'] == 0), 'pred_priority'] = 2
    y_test_combined.loc[(y_test_combined['pred_emergency'] == 0) & (y_test_combined['pred_important'] == 1), 'pred_priority'] = 3
    
    # Calculate metrics for the 4-class problem
    print("\nEvaluation for 4-class priority prediction:")
    print(classification_report(y_test_combined['true_priority'], y_test_combined['pred_priority']))
    
    # Create confusion matrix for the 4-class problem
    cm = confusion_matrix(y_test_combined['true_priority'], y_test_combined['pred_priority'])
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
               xticklabels=['Priority 1', 'Priority 2', 'Priority 3', 'Priority 4'],
               yticklabels=['Priority 1', 'Priority 2', 'Priority 3', 'Priority 4'])
    plt.xlabel('Predicted Priority')
    plt.ylabel('Actual Priority')
    plt.title('Confusion Matrix for 4-Class Priority Prediction')
    plt.show()
    
    return y_test_combined

def predict_with_imbalanced_models(emergency_model, important_model, new_data, numerical_features):
    """
    Use the trained models to predict priority for new network traffic
    """
    # Preprocess the new data
    X_new = new_data[numerical_features]
    
    # Make predictions with both models
    emergency_pred = emergency_model.predict(X_new)
    important_pred = important_model.predict(X_new)
    
    # Combine predictions
    pred_df = pd.DataFrame({
        'emergency': emergency_pred,
        'important': important_pred
    })
    
    # Map to priority classes
    pred_df['priority_class'] = 4  # Default: Not Emergency and Not Important
    pred_df.loc[(pred_df['emergency'] == 1) & (pred_df['important'] == 1), 'priority_class'] = 1
    pred_df.loc[(pred_df['emergency'] == 1) & (pred_df['important'] == 0), 'priority_class'] = 2
    pred_df.loc[(pred_df['emergency'] == 0) & (pred_df['important'] == 1), 'priority_class'] = 3
    
    # Map to priority labels
    priority_labels = {
        1: "Emergency and Important (highest priority)",
        2: "Emergency but Not Important",
        3: "Not Emergency but Important",
        4: "Not Emergency and Not Important (lowest priority)"
    }
    
    pred_df['priority_label'] = pred_df['priority_class'].map(priority_labels)
    
    # Combine with original data
    result = pd.concat([new_data.reset_index(drop=True), pred_df], axis=1)
    
    return result

def interpret_model(model, feature_names):
    """
    Interpret model feature importance
    """
    if hasattr(model, 'named_steps') and hasattr(model.named_steps['classifier'], 'feature_importances_'):
        importances = model.named_steps['classifier'].feature_importances_
        indices = np.argsort(importances)[::-1]
        
        # Print feature ranking
        print("Feature ranking:")
        for i in range(min(20, len(feature_names))):  # Show top 20 features or all if less than 20
            print(f"{i+1}. {feature_names[indices[i]]} ({importances[indices[i]]:.4f})")
        
        # Plot feature importance
        plt.figure(figsize=(12, 8))
        plt.title("Feature importances")
        plt.bar(range(min(20, len(feature_names))), 
                importances[indices[:20]],
                align="center")
        plt.xticks(range(min(20, len(feature_names))), 
                  [feature_names[i] for i in indices[:20]], 
                  rotation=90)
        plt.tight_layout()
        plt.show()

# Main execution
def main():
    # Sample execution - replace with your actual data path
    data_path = "hospital_network_data.csv"
    
    try:
        # Step 1: Load and preprocess data
        print("Step 1: Loading and preprocessing data...")
        df = load_and_preprocess_data(data_path)
        
        # Step 2: Extract features
        print("\nStep 2: Extracting features...")
        df, numerical_features = extract_features(df)
        
        # Step 3: Create enhanced target variables with more sophisticated rules
        print("\nStep 3: Creating enhanced target variables...")
        df = create_enhanced_target_variables(df)
        
        # Step 4: Train models for imbalanced data
        print("\nStep 4: Training models with handling for imbalanced data...")
        X = df[numerical_features]
        y = df[['emergency', 'important']]
        emergency_model, important_model, X_test, y_test = train_imbalanced_multitask_model(X, y)
        
        # Step 5: Evaluate the models
        print("\nStep 5: Evaluating the models...")
        evaluation_results = evaluate_imbalanced_models(emergency_model, important_model, X_test, y_test)
        
        # Step 6: Model interpretation
        print("\nStep 6: Interpreting the models...")
        print("Emergency model feature importance:")
        interpret_model(emergency_model, numerical_features)
        
        print("\nImportance model feature importance:")
        interpret_model(important_model, numerical_features)
        
        # Step 7: Make predictions on new data (in a real scenario)
        print("\nStep 7: Demonstrating prediction on new data...")
        # For demonstration, we'll use a small subset of the test data as "new" data
        new_data = X_test.iloc[:5].copy()
        predictions = predict_with_imbalanced_models(emergency_model, important_model, 
                                                    new_data, numerical_features)
        print("\nSample predictions:")
        print(predictions[['priority_class', 'priority_label']])
        
        print("\nModel training and evaluation completed successfully.")
        
    except Exception as e:
        print(f"An error occurred: {str(e)}")

if __name__ == "__main__":
    main()
