{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNf9q6SuoLUdtodv+GjiPrR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LavanyaShivamurthy/myCoLabLearing/blob/main/StructuredCode_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WwOR7_L5DXON"
      },
      "outputs": [],
      "source": [
        "# network_preprocessor.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "# %%\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class NetworkDataPreprocessor:\n",
        "    def __init__(self, filepath, low_memory=False):\n",
        "        self.filepath = filepath\n",
        "        self.low_memory = low_memory\n",
        "        self.df = None\n",
        "        self.selected_features = []\n",
        "        self.flow_metrics = {}\n",
        "\n",
        "    def load_data(self):\n",
        "        print(\"[INFO] Loading dataset...\")\n",
        "        self.df = pd.read_csv(\n",
        "            self.filepath,\n",
        "            low_memory=self.low_memory,\n",
        "            dtype={\n",
        "                'frame.time_delta': 'float64',\n",
        "                'frame.len': 'float64',\n",
        "                26: 'object', 28: 'object', 35: 'object'\n",
        "            }\n",
        "        )\n",
        "        print(f\"[INFO] Dataset shape: {self.df.shape}\")\n",
        "\n",
        "\n",
        "    def handle_missing_values(self):\n",
        "        # get columns with missig vaulues\n",
        "        missing_cols = self.df.columns[self.df.isnull().any()]\n",
        "        missing_stats = {'columns': {}}# # Initialize missing_stats dictionary\n",
        "        print(f\"Total missing values: {self.df.isnull().sum().sum()}\")\n",
        "        print(f\"Columns with missing values: {len(missing_cols)} out of {len(self.df.columns)}\")\n",
        "        self.df.drop_duplicates(inplace=True)\n",
        "        # Detailed report for columns with missing values\n",
        "        print(\"\\nMissing value analysis by column:\")\n",
        "        for col in missing_cols:\n",
        "          missing_count = self.df[col].isnull().sum()\n",
        "          missing_percent = (missing_count / len(self.df)) * 100\n",
        "          missing_stats['columns'][col] = {\n",
        "            'count': int(missing_count),\n",
        "            'percent': float(missing_percent)\n",
        "        }\n",
        "\n",
        "\n",
        "        # Row-level missing value analysis\n",
        "        missing_counts_per_row =self.df.isnull().sum(axis=1)\n",
        "        rows_with_missing = (missing_counts_per_row > 0).sum()\n",
        "        missing_stats['rows'] = {\n",
        "        'total_rows_with_missing': int(rows_with_missing),\n",
        "        'percent_rows_with_missing': float((rows_with_missing / len(self.df)) * 100),\n",
        "        'distribution': {}\n",
        "        }\n",
        "\n",
        "        # Distribution of missing values per row\n",
        "        value_counts = missing_counts_per_row.value_counts().sort_index()\n",
        "        print(\"\\nMissing value analysis by row:\")\n",
        "        print(f\"Rows with at least one missing value: {rows_with_missing} out of {len(self.df)} ({(rows_with_missing/len(self.df))*100:.2f}%)\")\n",
        "        print(\"\\nDistribution of missing values per row:\")\n",
        "        missing_stats['rows']['distribution'] = {}\n",
        "        percent=0.0\n",
        "        for count, frequency in value_counts.items():\n",
        "         if count > 0:  # Only show rows that have missing values\n",
        "            percent = (frequency / len(self.df)) * 100\n",
        "            missing_stats['rows']['distribution'][int(count)] = {\n",
        "                'frequency': int(frequency),\n",
        "                'percent': float(percent)\n",
        "            }\n",
        "         print(f\"  - {count} missing values: {frequency} rows ({percent:.2f}%)\")\n",
        "         # Print column names\n",
        "         print(\"Column Names:\")\n",
        "         print(self.df.columns)\n",
        "\n",
        "         #2. Duplicate and Null  removal\n",
        "         print(\"Number of duplciate rows:\",self.df.duplicated().sum())\n",
        "         #print(\"Null values in each column:\")\n",
        "         #print(df.isnull().sum())\n",
        "         print(\"Data frame before the removal\")\n",
        "         print(self.df)\n",
        "         if self.df.duplicated().sum()>0:\n",
        "            #df.drop_duplicates().dropna()# Null valeus  have meaning in this data set, so connot remove null\n",
        "            # for example tcp.connection.fin = 0 means that the FIN (Finish) flag in the TCP header is not set,\n",
        "            self.df.drop_duplicates()\n",
        "\n",
        "\n",
        "         print(\"Data frame after the removal\")\n",
        "         print(self.df)\n",
        "         # Print column names\n",
        "         print(\"Column Names:\")\n",
        "         print(self.df.columns)\n",
        "\n",
        "         # Replace invalid frame time deltas\n",
        "         # Ensure non-zero time deltas to prevent division by zero\n",
        "         epsilon = 1e-6\n",
        "         self.df['frame.time_delta'] = self.df['frame.time_delta'].apply(lambda x: epsilon if x <= 0 else x)\n",
        "         #  Ensure non-zero time deltas to prevent division by zero\n",
        "         print(f\"Updated dataset shape: {self.df.shape}\")\n",
        "\n",
        "    def calculate_bandwidth(self):\n",
        "       print(\"[INFO] Calculating bandwidth...\")\n",
        "       # Create flow groups\n",
        "\n",
        "       flow_groups = self.df.groupby(['ip.src', 'ip.dst', 'tcp.srcport', 'tcp.dstport'])\n",
        "       # Calculate bandwidth\n",
        "       bandwidth_data = []\n",
        "       for name, group in flow_groups:\n",
        "          if group['frame.time_delta'].sum() > 0:\n",
        "              bw = (group['frame.len'].sum() * 8) / group['frame.time_delta'].sum()\n",
        "          else:\n",
        "              bw = 0\n",
        "          # Create a record with the group keys and bandwidth value\n",
        "          src_ip, dst_ip, src_port, dst_port = name\n",
        "          bandwidth_data.append({\n",
        "          'ip.src': src_ip,\n",
        "          'ip.dst': dst_ip,\n",
        "          'tcp.srcport': src_port,\n",
        "          'tcp.dstport': dst_port,\n",
        "          'bandwidth_bps': bw\n",
        "          })\n",
        "\n",
        "       # Create a fresh DataFrame with the bandwidth data\n",
        "       flow_bandwidth = pd.DataFrame(bandwidth_data)\n",
        "\n",
        "       # Create a temporary key for merging to avoid duplicate column issues\n",
        "       self.df['temp_key'] = self.df['ip.src'].astype(str) + \"_\" + \\\n",
        "                      self.df['ip.dst'].astype(str) + \"_\" + \\\n",
        "                      self.df['tcp.srcport'].astype(str) + \"_\" + \\\n",
        "                      self.df['tcp.dstport'].astype(str)\n",
        "\n",
        "       flow_bandwidth['temp_key'] = flow_bandwidth['ip.src'].astype(str) + \"_\" + \\\n",
        "                            flow_bandwidth['ip.dst'].astype(str) + \"_\" + \\\n",
        "                            flow_bandwidth['tcp.srcport'].astype(str) + \"_\" + \\\n",
        "                            flow_bandwidth['tcp.dstport'].astype(str)\n",
        "\n",
        "       # Merge on the temporary key\n",
        "       self.df = self.df.merge(\n",
        "        flow_bandwidth[['temp_key', 'bandwidth_bps']],\n",
        "        on='temp_key',\n",
        "        how='left'\n",
        "       )\n",
        "\n",
        "       # Clean up by removing the temporary key\n",
        "       self.df.drop('temp_key', axis=1, inplace=True)\n",
        "       return flow_bandwidth\n",
        "\n",
        "\n",
        "    def calculate_flow_metrics(self):\n",
        "        print(\"[INFO] Calculating flow metrics...\")\n",
        "        # Use pd.notna instead of plt.plot_date.notna\n",
        "        self.df['flow_key_forward'] = self.df.apply(\n",
        "            lambda x: f\"{x['ip.src']}:{x['tcp.srcport']}-{x['ip.dst']}:{x['tcp.dstport']}\" if pd.notna(x['tcp.srcport']) else \"\", axis=1\n",
        "        )\n",
        "        # Use pd.notna instead of self.pd.notna (which is incorrect)\n",
        "        self.df['flow_key_backward'] = self.df.apply(\n",
        "            lambda x: f\"{x['ip.dst']}:{x['tcp.dstport']}-{x['ip.src']}:{x['tcp.srcport']}\" if pd.notna(x['tcp.dstport']) else \"\", axis=1\n",
        "        )\n",
        "\n",
        "        flow_stats = defaultdict(lambda: {'packet_count': 0, 'byte_count': 0, 'time_deltas': [], 'packet_sizes': []})\n",
        "\n",
        "        for _, row in self.df.iterrows():\n",
        "            key = row['flow_key_forward'] if row['flow_key_forward'] else row['flow_key_backward']\n",
        "            if key:\n",
        "                flow_stats[key]['packet_count'] += 1\n",
        "                flow_stats[key]['byte_count'] += row['frame.len']\n",
        "                flow_stats[key]['time_deltas'].append(row['frame.time_delta'])\n",
        "                flow_stats[key]['packet_sizes'].append(row['frame.len'])\n",
        "\n",
        "        self.flow_metrics = {k: {\n",
        "            'packet_count': v['packet_count'],\n",
        "            'byte_count': v['byte_count'],\n",
        "            'avg_packet_size': np.mean(v['packet_sizes']),\n",
        "            'std_packet_size': np.std(v['packet_sizes']),\n",
        "            'avg_time_delta': np.mean(v['time_deltas']),\n",
        "            'jitter': np.std(v['time_deltas']),\n",
        "            'flow_duration': sum(v['time_deltas']),\n",
        "            'avg_bandwidth': v['byte_count'] / sum(v['time_deltas']) if sum(v['time_deltas']) else 0\n",
        "        } for k, v in flow_stats.items() if v['packet_count'] >= 3}\n",
        "        print(\"[INFO] Flow metrics calculated.\")\n",
        "        # Add flow metrics to dataframe\n",
        "        df['flow_id'] = df.apply(\n",
        "            lambda x: x['flow_key_forward'] if x['flow_key_forward'] in flow_metrics\n",
        "                     else (x['flow_key_backward'] if x['flow_key_backward'] in flow_metrics else \"\"),\n",
        "            axis=1\n",
        "        )\n",
        "\n",
        "    def feature_engineering(self):\n",
        "        print(\"[INFO] Starting feature engineering...\")\n",
        "        #self.calculate_bandwidth()\n",
        "        # First calculate bandwidth\n",
        "        bandwidth_df = self.calculate_bandwidth()\n",
        "        outlier_df = self.detect_bandwidth_outliers(bandwidth_df, method='iqr', threshold=1.5)\n",
        "        print(outlier_df)\n",
        "        #Then detect outliers\n",
        "        outlier_df = self.detect_bandwidth_outliers(bandwidth_df, method='iqr', threshold=1.5)\n",
        "        # Handle outliers based on your needs\n",
        "        cleaned_df = self.handle_bandwidth_outliers(outlier_df, method='cap')\n",
        "        self.detect_mqtt()\n",
        "        self.calculate_flow_metrics()\n",
        "        self.assess_emergency_importance()\n",
        "        self.assign_eisenhower_category()\n",
        "        self.select_features()\n",
        "\n",
        "    def detect_mqtt(self):\n",
        "        print(\"[INFO] Detecting MQTT traffic...\")\n",
        "        self.df['is_mqtt'] = 0\n",
        "        if 'mqtt.topic' in self.df.columns:\n",
        "            self.df['mqtt.topic'] = self.df['mqtt.topic'].astype(str)\n",
        "            self.df['is_mqtt'] = self.df['mqtt.topic'].apply(lambda x: 0 if x in [\"0\", \"nan\", None] else 1)\n",
        "\n",
        "    def assess_importance(self, df, flow_metrics=None):\n",
        "      \"\"\"\n",
        "      Assess importance level of network traffic based on multiple factors:\n",
        "      - Protocol ports (critical databases, business apps)\n",
        "      - IP address ranges (internal networks)\n",
        "      - Application protocols (MQTT, business apps)\n",
        "      - Data transfer sizes (large transfers)\n",
        "      \"\"\"\n",
        "      print(\"[INFO] Assessing Importance...\")\n",
        "\n",
        "      # Initialize importance level column\n",
        "      df['importance_level'] = 0  # 0: not important, 1: somewhat important, 2: highly important\n",
        "\n",
        "      # Define port columns to check\n",
        "      port_cols = ['tcp.srcport', 'tcp.dstport', 'udp.srcport', 'udp.dstport']\n",
        "      port_cols = [col for col in port_cols if col in df.columns]\n",
        "\n",
        "      # Debug: Initial state\n",
        "      print(\"Before importance assignment:\")\n",
        "      print(df['importance_level'].value_counts())\n",
        "\n",
        "      # 1. Protocol-based importance assessment\n",
        "      importance_protocols = {\n",
        "          # Business critical applications\n",
        "          'critical_ports': [1433, 1521, 3306, 5432, 6379, 27017, 7000, 7001, 9042],  # Databases, key infrastructure\n",
        "          'important_ports': [22, 23, 25, 110, 143, 465, 587, 993, 995, 389, 636],  # SSH, Email, LDAP\n",
        "          'business_web_ports': [8080, 8443, 9000, 9090, 8008, 8888]  # Business web apps\n",
        "      }\n",
        "\n",
        "      if port_cols:\n",
        "          # Create tiered importance based on protocol groups\n",
        "          high_importance_ports = importance_protocols['critical_ports']\n",
        "          medium_importance_ports = importance_protocols['important_ports'] + importance_protocols['business_web_ports']\n",
        "\n",
        "          # Apply importance levels based on ports\n",
        "          for col in port_cols:\n",
        "              if col in df.columns:\n",
        "                  # Mark high importance protocols (level 2)\n",
        "                  high_imp_mask = df[col].isin(high_importance_ports)\n",
        "                  df.loc[high_imp_mask & (df['importance_level'] < 2), 'importance_level'] = 2\n",
        "\n",
        "                  # Mark medium importance protocols (level 1)\n",
        "                  med_imp_mask = df[col].isin(medium_importance_ports)\n",
        "                  df.loc[med_imp_mask & (df['importance_level'] < 1), 'importance_level'] = 1\n",
        "\n",
        "      # Debug: After port-based assessment\n",
        "      print(\"\\nAfter port-based importance:\")\n",
        "      if any(df['importance_level'] > 0):\n",
        "          for col in port_cols:\n",
        "              if col in df.columns:\n",
        "                  port_counts = df[df['importance_level'] > 0][col].value_counts()\n",
        "                  if not port_counts.empty:\n",
        "                      print(f\"{col}: {port_counts.head()}\")\n",
        "      \"\"\"\n",
        "      # 2. IP-based importance assessment\n",
        "      def is_important_ip(ip):\n",
        "          #Check if IP belongs to important internal networks\n",
        "          if pd.isna(ip) or not isinstance(ip, str):\n",
        "              return False\n",
        "          # Common private IP ranges\n",
        "          return (ip.startswith(\"10.\") or\n",
        "                  ip.startswith(\"192.168.\") or\n",
        "                  ip.startswith(\"172.16.\") or\n",
        "                  ip.startswith(\"172.17.\") or\n",
        "                  ip.startswith(\"172.18.\") or\n",
        "                  ip.startswith(\"172.19.\") or\n",
        "                  ip.startswith(\"172.20.\") or\n",
        "                  ip.startswith(\"172.21.\") or\n",
        "                  ip.startswith(\"172.22.\") or\n",
        "                  ip.startswith(\"172.23.\") or\n",
        "                  ip.startswith(\"172.24.\") or\n",
        "                  ip.startswith(\"172.25.\") or\n",
        "                  ip.startswith(\"172.26.\") or\n",
        "                  ip.startswith(\"172.27.\") or\n",
        "                  ip.startswith(\"172.28.\") or\n",
        "                  ip.startswith(\"172.29.\") or\n",
        "                  ip.startswith(\"172.30.\") or\n",
        "                  ip.startswith(\"172.31.\"))\n",
        "      \"\"\"\n",
        "      def is_important_ip(ip):\n",
        "        if isinstance(ip, str):\n",
        "          return ip.startswith(\"10.\") or ip.startswith(\"192.168.\") or ip.startswith(\"172.16.\")\n",
        "          return False\n",
        "        df['is_important_ip'] = df['ip.src'].apply(is_important_ip) | df['ip.dst'].apply(is_important_ip)\n",
        "        df.loc[df['is_important_ip'], 'importance_level'] = 1\n",
        "\n",
        "      # Apply IP-based importance if columns exist\n",
        "      if 'ip.src' in df.columns and 'ip.dst' in df.columns:\n",
        "          df['is_important_ip'] = df['ip.src'].apply(is_important_ip) | df['ip.dst'].apply(is_important_ip)\n",
        "          df.loc[df['is_important_ip'] & (df['importance_level'] < 1), 'importance_level'] = 1\n",
        "\n",
        "          # Debug: After IP-based assessment\n",
        "          print(\"\\nAfter IP-based importance:\")\n",
        "          if 'is_important_ip' in df.columns and df['is_important_ip'].any():\n",
        "              ip_counts = df[df['is_important_ip']]['ip.src'].value_counts()\n",
        "              if not ip_counts.empty:\n",
        "                  print(f\"Important source IPs: {ip_counts.head()}\")\n",
        "\n",
        "      # 3. MQTT topic-based importance\n",
        "      if 'mqtt.topic' in df.columns:\n",
        "          # Ensure mqtt.topic is string and handle NaN values\n",
        "          df['mqtt.topic'] = df['mqtt.topic'].fillna('').astype(str).str.lower()\n",
        "\n",
        "          important_topics = ['temperature', 'humidity', 'solar', 'rad', 'motion', 'door', 'window',\n",
        "                            'alarm', 'security', 'sensor', 'control', 'critical']\n",
        "\n",
        "          # Create boolean mask for important MQTT topics\n",
        "          mqtt_mask = df['mqtt.topic'].apply(\n",
        "              lambda x: any(topic in x for topic in important_topics) if x else False\n",
        "          )\n",
        "          df.loc[mqtt_mask & (df['importance_level'] < 1), 'importance_level'] = 1\n",
        "\n",
        "          # Debug: After MQTT assessment\n",
        "          print(\"\\nAfter MQTT-based importance:\")\n",
        "          if mqtt_mask.any():\n",
        "              topic_counts = df[mqtt_mask]['mqtt.topic'].value_counts()\n",
        "              if not topic_counts.empty:\n",
        "                  print(f\"Important MQTT topics: {topic_counts.head()}\")\n",
        "\n",
        "      # 4. Application-level importance (protocol detection)\n",
        "      critical_protocols = ['ldap', 'kerberos', 'mssql', 'mysql', 'oracle', 'postgresql', 'mongodb']\n",
        "      important_protocols = ['ssh', 'telnet', 'smtp', 'imap', 'pop', 'dns', 'ntp']\n",
        "\n",
        "      # Check for critical protocols\n",
        "      for protocol in critical_protocols:\n",
        "          protocol_cols = [col for col in df.columns if protocol in col.lower()]\n",
        "          if protocol_cols:\n",
        "              # Mark as critical if protocol data is present\n",
        "              for col in protocol_cols:\n",
        "                  protocol_mask = df[col].notna() & (df[col] != 0) & (df[col] != '')\n",
        "                  df.loc[protocol_mask & (df['importance_level'] < 2), 'importance_level'] = 2\n",
        "\n",
        "      # Check for important protocols\n",
        "      for protocol in important_protocols:\n",
        "          protocol_cols = [col for col in df.columns if protocol in col.lower()]\n",
        "          if protocol_cols:\n",
        "              # Mark as important if protocol data is present\n",
        "              for col in protocol_cols:\n",
        "                  protocol_mask = df[col].notna() & (df[col] != 0) & (df[col] != '')\n",
        "                  df.loc[protocol_mask & (df['importance_level'] < 1), 'importance_level'] = 1\n",
        "\n",
        "      # 5. Data size-based importance (large data transfers)\n",
        "      if 'flow_id' in df.columns and flow_metrics is not None:\n",
        "          # Add flow byte count information\n",
        "          if isinstance(flow_metrics, dict):\n",
        "              df['flow_byte_count'] = df['flow_id'].map(\n",
        "                  {k: v.get('byte_count', 0) if isinstance(v, dict) else 0\n",
        "                  for k, v in flow_metrics.items()}\n",
        "              ).fillna(0)\n",
        "          else:\n",
        "              # If flow_metrics is already a DataFrame\n",
        "              try:\n",
        "                  flow_df = pd.DataFrame(flow_metrics).T\n",
        "                  if 'byte_count' in flow_df.columns:\n",
        "                      df = df.merge(flow_df[['byte_count']].rename(columns={'byte_count': 'flow_byte_count'}),\n",
        "                                  left_on='flow_id', right_index=True, how='left')\n",
        "                      df['flow_byte_count'] = df['flow_byte_count'].fillna(0)\n",
        "              except Exception as e:\n",
        "                  print(f\"[WARNING] Could not process flow_metrics: {e}\")\n",
        "                  df['flow_byte_count'] = 0\n",
        "\n",
        "          # Mark large transfers as important (reduced threshold to 500KB)\n",
        "          if 'flow_byte_count' in df.columns:\n",
        "              large_transfer_mask = df['flow_byte_count'] > 500000\n",
        "              df.loc[large_transfer_mask & (df['importance_level'] < 1), 'importance_level'] = 1\n",
        "\n",
        "      # Final debug output\n",
        "      print(\"\\nFinal importance distribution:\")\n",
        "      importance_counts = df['importance_level'].value_counts().sort_index()\n",
        "      print(f\"  - Not important (level 0): {importance_counts.get(0, 0)} packets\")\n",
        "      print(f\"  - Somewhat important (level 1): {importance_counts.get(1, 0)} packets\")\n",
        "      print(f\"  - Highly important (level 2): {importance_counts.get(2, 0)} packets\")\n",
        "\n",
        "      # Clean up temporary columns\n",
        "      temp_cols = ['is_important_ip']\n",
        "      for col in temp_cols:\n",
        "          if col in df.columns:\n",
        "              df.drop(col, axis=1, inplace=True)\n",
        "\n",
        "      print(\"[INFO] Importance assessment complete.\")\n",
        "      return df\n",
        "\n",
        "\n",
        "    def assess_emergency(self, df, flow_metrics=None):\n",
        "\n",
        "        \"\"\"\n",
        "        Assess emergency level of network traffic based on multiple factors:\n",
        "        - Protocol ports (critical, urgent, standard)\n",
        "        - TCP flags (URG flag)\n",
        "        - QoS markings (DSCP values)\n",
        "        - Flow characteristics (regularity, jitter)\n",
        "        - Real-time traffic patterns\n",
        "        \"\"\"\n",
        "        print(\"[INFO] Assessing Emergency...\")\n",
        "        df['emergency_level'] = 0  # 0: not emergency, 1: somewhat emergency, 2: high emergency\n",
        "\n",
        "        # Protocol-based detection for emergency assessment\n",
        "        emergency_sensitive_protocols = {\n",
        "            # Real-time communications (VoIP, video conferencing)\n",
        "            'critical_ports': [5060, 5061, 16384, 16394, 10000, 10001, 3478, 3479, 5004, 5005],  # SIP, RTP, STUN\n",
        "            'urgent_ports': [1935, 8554, 554, 8000, 8080, 8443, 3074, 3075, 3076, 27015],  # Streaming, Gaming\n",
        "            'standard_ports': [3389, 5900, 5800, 4172, 80, 443, 8443]  # RDP, VNC, HTTP(S)\n",
        "        }\n",
        "\n",
        "        # Check which port columns exist in the dataframe\n",
        "        port_cols = ['tcp.srcport', 'tcp.dstport', 'udp.srcport', 'udp.dstport']\n",
        "        port_cols = [col for col in port_cols if col in df.columns]\n",
        "\n",
        "        if port_cols:\n",
        "            # Create tiered emergency based on protocol groups\n",
        "            high_emergency_ports = emergency_sensitive_protocols['critical_ports']\n",
        "            # Fixed: Remove quotes around 'standard_ports' and combine lists properly\n",
        "            medium_emergency_ports = emergency_sensitive_protocols['urgent_ports'] + emergency_sensitive_protocols['standard_ports']\n",
        "\n",
        "            # Apply emergency levels based on ports\n",
        "            for col in port_cols:\n",
        "                if col in df.columns:\n",
        "                    # Mark high emergency protocols (level 2)\n",
        "                    high_emerg_mask = df[col].isin(high_emergency_ports)\n",
        "                    df.loc[high_emerg_mask & (df['emergency_level'] < 2), 'emergency_level'] = 2\n",
        "\n",
        "                    # Mark medium emergency protocols (level 1)\n",
        "                    med_emerg_mask = df[col].isin(medium_emergency_ports)\n",
        "                    df.loc[med_emerg_mask & (df['emergency_level'] < 1), 'emergency_level'] = 1\n",
        "\n",
        "        # TCP flags for potential emergency\n",
        "        if 'tcp.flags.urg' in df.columns:\n",
        "            # URG flag often indicates emergency traffic\n",
        "            df.loc[df['tcp.flags.urg'] == 1, 'emergency_level'] = 2\n",
        "\n",
        "        # Check for QoS markings for emergency assessment\n",
        "        if 'ip.dsfield' in df.columns:\n",
        "            # Check for expedited forwarding (EF) or voice admit DSCP values\n",
        "            ef_dscp_values = [46, 44, 45]  # EF (46), Voice-Admit (44,45)\n",
        "            af_dscp_values = [26, 28, 30, 32, 34, 36, 38]  # Assured Forwarding values\n",
        "\n",
        "            # High priority QoS markings indicate emergency\n",
        "            df.loc[df['ip.dsfield'].isin(ef_dscp_values), 'emergency_level'] = 2\n",
        "\n",
        "            # Medium priority QoS markings\n",
        "            medium_qos_mask = df['ip.dsfield'].isin(af_dscp_values)\n",
        "            df.loc[medium_qos_mask & (df['emergency_level'] < 1), 'emergency_level'] = 1\n",
        "\n",
        "        # Flow-level analysis for emergency assessment\n",
        "        if 'flow_regularity' in df.columns and 'flow_jitter' in df.columns:\n",
        "            # Flows with high regularity and low jitter may indicate emergency traffic\n",
        "            if 'flow_packet_count' in df.columns:\n",
        "                high_regularity_flows = (\n",
        "                    (df['flow_regularity'] > 0.8) &\n",
        "                    (df['flow_jitter'] < 0.01) &\n",
        "                    (df['flow_packet_count'] >= 10)\n",
        "                )\n",
        "                df.loc[high_regularity_flows & (df['emergency_level'] < 2), 'emergency_level'] = 2\n",
        "\n",
        "                # Moderately regular flows\n",
        "                med_regularity_flows = (\n",
        "                    (df['flow_regularity'] > 0.6) &\n",
        "                    (df['flow_jitter'] < 0.05) &\n",
        "                    (df['flow_packet_count'] >= 5)\n",
        "                )\n",
        "                df.loc[med_regularity_flows & (df['emergency_level'] < 1), 'emergency_level'] = 1\n",
        "\n",
        "        # Real-time traffic pattern detection\n",
        "        if all(col in df.columns for col in ['frame.len', 'frame.time_delta', 'flow_id']):\n",
        "            # Check if flow_metrics is provided and merge if needed\n",
        "            if flow_metrics is not None and 'flow_packet_count' not in df.columns:\n",
        "                # Convert flow_metrics to DataFrame if it's a dictionary\n",
        "                if isinstance(flow_metrics, dict):\n",
        "                    flow_df = pd.DataFrame(flow_metrics).T\n",
        "                    flow_df = flow_df.rename(columns={'packet_count': 'flow_packet_count'})\n",
        "                    df = df.merge(flow_df[['flow_packet_count']],\n",
        "                                left_on='flow_id', right_index=True, how='left')\n",
        "\n",
        "            # Check if we now have flow_packet_count column\n",
        "            if 'flow_packet_count' in df.columns:\n",
        "                # Very stringent pattern for small, regular, bidirectional packets (real-time traffic)\n",
        "                realtime_traffic = (\n",
        "                    (df['frame.len'] < 150) &        # Smaller packets\n",
        "                    (df['frame.time_delta'] > 0.005) &  # Not too fast\n",
        "                    (df['frame.time_delta'] < 0.04) &   # Not too slow (25+ packets per second)\n",
        "                    (df['flow_packet_count'] > 10)      # Part of an established flow\n",
        "                )\n",
        "                df.loc[realtime_traffic & (df['emergency_level'] < 2), 'emergency_level'] = 2\n",
        "\n",
        "        # Print summary statistics\n",
        "        emergency_counts = df['emergency_level'].value_counts().sort_index()\n",
        "        print(f\"[INFO] Emergency Assessment Complete:\")\n",
        "        print(f\"  - Normal traffic (level 0): {emergency_counts.get(0, 0)} packets\")\n",
        "        print(f\"  - Medium emergency (level 1): {emergency_counts.get(1, 0)} packets\")\n",
        "        print(f\"  - High emergency (level 2): {emergency_counts.get(2, 0)} packets\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def assign_eisenhower_category(self, df=None):\n",
        "      \"\"\"\n",
        "      Assign Eisenhower Matrix categories based on emergency and importance levels.\n",
        "      Uses the results from assess_emergency() and assess_importance() functions.\n",
        "\n",
        "      Eisenhower Matrix Categories:\n",
        "      - Emergency and Important (Quadrant 1): Do First\n",
        "      - Not Emergency but Important (Quadrant 2): Schedule\n",
        "      - Emergency but Not Important (Quadrant 3): Delegate\n",
        "      - Not Emergency and Not Important (Quadrant 4): Eliminate\n",
        "\n",
        "      Args:\n",
        "          df: DataFrame with emergency_level and importance_level columns\n",
        "\n",
        "      Returns:\n",
        "          DataFrame with eisenhower_category column added\n",
        "      \"\"\"\n",
        "      print(\"[INFO] Assigning Eisenhower categories...\")\n",
        "\n",
        "      # Use provided df or self.df\n",
        "      if df is None:\n",
        "          if not hasattr(self, 'df'):\n",
        "              raise ValueError(\"No DataFrame provided and self.df not found\")\n",
        "          df = self.df\n",
        "      else:\n",
        "          self.df = df\n",
        "\n",
        "      # Verify required columns exist\n",
        "      required_cols = ['emergency_level', 'importance_level']\n",
        "      missing_cols = [col for col in required_cols if col not in df.columns]\n",
        "      if missing_cols:\n",
        "          raise ValueError(f\"Missing required columns: {missing_cols}. \"\n",
        "                          \"Please run assess_emergency() and assess_importance() first.\")\n",
        "\n",
        "      # Initialize with default category (Quadrant 4)\n",
        "      df['eisenhower_category'] = 'Not Emergency and Not Important'\n",
        "\n",
        "      # Quadrant 1: Emergency and Important (Do First)\n",
        "      # High priority: immediate action required\n",
        "      df.loc[\n",
        "          (df['emergency_level'] >= 1) & (df['importance_level'] >= 1),\n",
        "          'eisenhower_category'\n",
        "      ] = 'Emergency and Important'\n",
        "\n",
        "      # Quadrant 2: Not Emergency but Important (Schedule)\n",
        "      # Important but not urgent: plan and schedule\n",
        "      df.loc[\n",
        "          (df['emergency_level'] == 0) & (df['importance_level'] >= 1),\n",
        "          'eisenhower_category'\n",
        "      ] = 'Not Emergency but Important'\n",
        "\n",
        "      # Quadrant 3: Emergency but Not Important (Delegate)\n",
        "      # Urgent but not important: can be delegated\n",
        "      df.loc[\n",
        "          (df['emergency_level'] >= 1) & (df['importance_level'] == 0),\n",
        "          'eisenhower_category'\n",
        "      ] = 'Emergency but Not Important'\n",
        "\n",
        "      # Quadrant 4: Not Emergency and Not Important (Eliminate)\n",
        "      # Neither urgent nor important: eliminate or minimize\n",
        "      # Already set as default, no additional assignment needed\n",
        "\n",
        "      # Generate summary statistics\n",
        "      category_counts = df['eisenhower_category'].value_counts()\n",
        "      total_packets = len(df)\n",
        "\n",
        "      print(\"[INFO] Eisenhower Matrix Distribution:\")\n",
        "      print(f\"  Quadrant 1 - Emergency and Important: {category_counts.get('Emergency and Important', 0)} packets ({category_counts.get('Emergency and Important', 0)/total_packets*100:.1f}%)\")\n",
        "      print(f\"  Quadrant 2 - Not Emergency but Important: {category_counts.get('Not Emergency but Important', 0)} packets ({category_counts.get('Not Emergency but Important', 0)/total_packets*100:.1f}%)\")\n",
        "      print(f\"   Quadrant 3 - Emergency but Not Important: {category_counts.get('Emergency but Not Important', 0)} packets ({category_counts.get('Emergency but Not Important', 0)/total_packets*100:.1f}%)\")\n",
        "      print(f\"  Quadrant 4 - Not Emergency and Not Important: {category_counts.get('Not Emergency and Not Important', 0)} packets ({category_counts.get('Not Emergency and Not Important', 0)/total_packets*100:.1f}%)\")\n",
        "\n",
        "      # Additional analysis: Show breakdown by emergency and importance levels\n",
        "      print(\"\\n[INFO] Detailed Level Breakdown:\")\n",
        "      level_breakdown = df.groupby(['emergency_level', 'importance_level']).size().reset_index(name='count')\n",
        "      for _, row in level_breakdown.iterrows():\n",
        "          emergency = row['emergency_level']\n",
        "          importance = row['importance_level']\n",
        "          count = row['count']\n",
        "          percentage = (count / total_packets) * 100\n",
        "          print(f\"  Emergency Level {emergency}, Importance Level {importance}: {count} packets ({percentage:.1f}%)\")\n",
        "\n",
        "      print(\"[INFO] Eisenhower categorization complete.\")\n",
        "      return df\n",
        "\n",
        "\n",
        "    # Helper function to run complete analysis pipeline\n",
        "    def run_complete_analysis(self, df, flow_metrics=None):\n",
        "        \"\"\"\n",
        "        Run the complete analysis pipeline: emergency assessment, importance assessment,\n",
        "        and Eisenhower categorization.\n",
        "\n",
        "        Args:\n",
        "            df: Input DataFrame with network traffic data\n",
        "            flow_metrics: Flow metrics dictionary (optional)\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with all analysis columns added\n",
        "        \"\"\"\n",
        "        print(\"[INFO] Starting complete network traffic analysis...\")\n",
        "\n",
        "        # Step 1: Assess emergency level\n",
        "        df = self.assess_emergency(df, flow_metrics)\n",
        "\n",
        "        # Step 2: Assess importance level\n",
        "        df = self.assess_importance(df, flow_metrics)\n",
        "\n",
        "        # Step 3: Assign Eisenhower categories\n",
        "        df = self.assign_eisenhower_category(df)\n",
        "\n",
        "        print(\"[INFO] Complete analysis finished.\")\n",
        "        return df\n",
        "\n",
        "\n",
        "    def select_features(self):\n",
        "        print(\"[INFO] Selecting final features...\")\n",
        "        # Example feature selection\n",
        "        print(\"[INFO] Available features:\", self.df.columns)\n",
        "        possible_features = ['frame.time_delta', 'frame.len', 'bandwidth_bps', 'is_mqtt']\n",
        "        self.selected_features = [feat for feat in possible_features if feat in self.df.columns]\n",
        "        print(\"[INFO] Selected features:\", self.selected_features)\n",
        "\n",
        "    def get_preprocessed_data(self):\n",
        "        return self.df, self.selected_features\n",
        "\n",
        "    def detect_bandwidth_outliers(self, bandwidth_df, method='iqr', threshold=1.5):\n",
        "      \"\"\"\n",
        "      Detect outliers in bandwidth data.\n",
        "\n",
        "      Parameters:\n",
        "      -----------\n",
        "      bandwidth_df : DataFrame\n",
        "      DataFrame containing bandwidth values\n",
        "      method : str, default='iqr'\n",
        "      Method for outlier detection. Options: 'iqr', 'zscore', 'percentile'\n",
        "      threshold : float, default=1.5\n",
        "      Threshold for outlier detection (IQR multiplier or z-score)\n",
        "      Returns:\n",
        "      --------\n",
        "      DataFrame with outlier flag\n",
        "      \"\"\"\n",
        "      print(\"[INFO] Detecting outliers in bandwidth data...\")\n",
        "      print(f\"Bandwidth data shape: {bandwidth_df.shape}\")\n",
        "      result = bandwidth_df.copy()\n",
        "\n",
        "      if method == 'iqr':\n",
        "        # IQR method\n",
        "        Q1 = result['bandwidth_bps'].quantile(0.25)\n",
        "        Q3 = result['bandwidth_bps'].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "\n",
        "        lower_bound = Q1 - threshold * IQR\n",
        "        upper_bound = Q3 + threshold * IQR\n",
        "\n",
        "        # Flag outliers\n",
        "        result['is_outlier'] = (result['bandwidth_bps'] < lower_bound) | (result['bandwidth_bps'] > upper_bound)\n",
        "\n",
        "        # Optionally add bounds for reference\n",
        "        result['lower_bound'] = lower_bound\n",
        "        result['upper_bound'] = upper_bound\n",
        "\n",
        "      elif method == 'zscore':\n",
        "        # Z-score method\n",
        "        from scipy import stats\n",
        "        z_scores = stats.zscore(result['bandwidth_bps'], nan_policy='omit')\n",
        "        result['is_outlier'] = abs(z_scores) > threshold\n",
        "\n",
        "      elif method == 'percentile':\n",
        "        # Percentile method\n",
        "        lower_bound = result['bandwidth_bps'].quantile(0.01)  # Bottom 1%\n",
        "        upper_bound = result['bandwidth_bps'].quantile(0.99)  # Top 1%\n",
        "\n",
        "        result['is_outlier'] = (result['bandwidth_bps'] < lower_bound) | (result['bandwidth_bps'] > upper_bound)\n",
        "        result['lower_bound'] = lower_bound\n",
        "        result['upper_bound'] = upper_bound\n",
        "\n",
        "       # Count outliers\n",
        "      num_outliers = result['is_outlier'].sum()\n",
        "      print(f\"Detected {num_outliers} outliers out of {len(result)} flows ({num_outliers/len(result):.2%})\")\n",
        "\n",
        "      return result\n",
        "\n",
        "    def handle_bandwidth_outliers(self, bandwidth_df, method='cap'):\n",
        "      \"\"\"\n",
        "      Handle outliers in bandwidth data.\n",
        "\n",
        "      Parameters:\n",
        "      -----------\n",
        "      bandwidth_df : DataFrame\n",
        "      DataFrame containing bandwidth values and outlier flags\n",
        "      method : str, default='cap'\n",
        "      Method for handling outliers. Options: 'cap', 'remove', 'log'\n",
        "\n",
        "      Returns:\n",
        "      --------\n",
        "      DataFrame with handled outliers\n",
        "      \"\"\"\n",
        "      if 'is_outlier' not in bandwidth_df.columns:\n",
        "        print(\"No outlier flags found. Run detect_bandwidth_outliers first.\")\n",
        "        return bandwidth_df\n",
        "\n",
        "      result = bandwidth_df.copy()\n",
        "\n",
        "      if method == 'cap':\n",
        "        # Cap outliers at bounds\n",
        "        if 'lower_bound' in result.columns and 'upper_bound' in result.columns:\n",
        "            result.loc[result['is_outlier'], 'bandwidth_bps_handled'] = result.loc[result['is_outlier']].apply(\n",
        "                lambda x: min(x['upper_bound'], max(x['lower_bound'], x['bandwidth_bps'])),\n",
        "                axis=1\n",
        "            )\n",
        "            # Keep original values for non-outliers\n",
        "            result.loc[~result['is_outlier'], 'bandwidth_bps_handled'] = result.loc[~result['is_outlier'], 'bandwidth_bps']\n",
        "        else:\n",
        "            print(\"Bounds not found. Recalculating...\")\n",
        "            # Recalculate bounds\n",
        "            Q1 = result['bandwidth_bps'].quantile(0.25)\n",
        "            Q3 = result['bandwidth_bps'].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            lower_bound = Q1 - 1.5 * IQR\n",
        "            upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "            # Cap values\n",
        "            result['bandwidth_bps_handled'] = result['bandwidth_bps'].clip(lower=lower_bound, upper=upper_bound)\n",
        "\n",
        "      elif method == 'remove':\n",
        "        # Remove outliers\n",
        "        result = result[~result['is_outlier']].copy()\n",
        "\n",
        "      elif method == 'log':\n",
        "        # Log transform\n",
        "        import numpy as np\n",
        "        result['bandwidth_bps_handled'] = np.log1p(result['bandwidth_bps'])\n",
        "\n",
        "      return result\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NEW: Importance assessment (separate from emergency)\n",
        "df['importance_level'] = 0  # 0: not important, 1: somewhat important, 2: highly important  # This line creates the 'importance_level' column\n",
        "#debuging\n",
        "print(\"Before importance assignment:\")\n",
        "print(df['importance_level'].value_counts())\n",
        "# Debugging each condition separately\n",
        "print(\"\\nPort-based importance:\")\n",
        "print(df[df['importance_level'] > 0]['tcp.srcport'].value_counts())\n",
        "\n",
        "print(\"\\nIP-based importance:\")\n",
        "print(df[df['importance_level'] > 0]['ip.src'].value_counts())\n",
        "\n",
        "print(\"\\nApplication-level importance:\")\n",
        "print(df[df['importance_level'] > 0]['mqtt.topic'].value_counts())\n",
        "\n",
        "print(\"\\nFinal importance distribution:\")\n",
        "print(df['importance_level'].value_counts())\n",
        "\n",
        "# --- FIX 1: Broaden IP-based filtering ---\n",
        "# Instead of strict prefixes, allow more ranges\n",
        "def is_important_ip(ip):\n",
        "  if isinstance(ip, str):\n",
        "    return ip.startswith(\"10.\") or ip.startswith(\"192.168.\") or ip.startswith(\"172.16.\")\n",
        "  return False\n",
        "\n",
        "df['is_important_ip'] = df['ip.src'].apply(is_important_ip) | df['ip.dst'].apply(is_important_ip)\n",
        "df.loc[df['is_important_ip'], 'importance_level'] = 1\n",
        "\n",
        "# --- FIX 2: Expand MQTT topic filtering ---\n",
        "if 'mqtt.topic' in df.columns:\n",
        "  df['mqtt.topic'] = df['mqtt.topic'].astype(str).str.lower()\n",
        "  important_topics = ['temperature', 'humidity', 'solar rad', 'motion', 'door', 'window']  # Expanded topics\n",
        "  df['is_important_mqtt'] = df['mqtt.topic'].apply(lambda x: any(topic in x for topic in important_topics))\n",
        "  df.loc[df['is_important_mqtt'], 'importance_level'] = 1\n",
        "\n",
        "# --- FIX 3: Adjust large data transfer detection ---\n",
        "if 'flow_id' in df.columns:\n",
        "  df = df.merge(pd.DataFrame(flow_metrics).T.rename(columns={'byte_count': 'flow_byte_count'}),\n",
        "              left_on='flow_id', right_index=True, how='left')\n",
        "  df['flow_byte_count'] = df['flow_byte_count'].fillna(0)\n",
        "  df['is_large_transfer'] = df['flow_byte_count'] > 500000  # Reduced threshold from 1M to 500K bytes\n",
        "  df.loc[df['is_large_transfer'], 'importance_level'] = 1\n",
        "\n",
        "# Re-run importance distribution check\n",
        "print(\"\\nUpdated importance distribution:\")\n",
        "print(df['importance_level'].value_counts())\n",
        "# NEW: Importance assessment (separate from emergency)\n",
        "df['importance_level'] = 0  # 0: not important, 1: somewhat important, 2: highly important\n",
        "\n",
        "# 1. Protocol-based importance assessment\n",
        "importance_protocols = {\n",
        "    # Business critical applications\n",
        "    'critical_ports': [1433, 1521, 3306, 5432, 6379, 27017, 7000, 7001, 9042],  # Databases, key infrastructure\n",
        "    'important_ports': [22, 23, 25, 110, 143, 465, 587, 993, 995, 389, 636],  # SSH, Email, LDAP\n",
        "    'business_web_ports': [8080, 8443, 9000, 9090, 8008, 8888]  # Business web apps\n",
        "}\n",
        "\n",
        "if port_cols:\n",
        "    # Create tiered importance based on protocol groups\n",
        "    high_importance_ports = importance_protocols['critical_ports']\n",
        "    medium_importance_ports = importance_protocols['important_ports'] + importance_protocols['business_web_ports']\n",
        "\n",
        "    # Apply importance levels based on ports\n",
        "    for col in port_cols:\n",
        "        if col in df.columns:\n",
        "            # Mark high importance protocols (level 2)\n",
        "            high_imp_mask = df[col].isin(high_importance_ports)\n",
        "            df.loc[high_imp_mask & (df['importance_level'] < 2), 'importance_level'] = 2\n",
        "\n",
        "            # Mark medium importance protocols (level 1)\n",
        "            med_imp_mask = df[col].isin(medium_importance_ports)\n",
        "            df.loc[med_imp_mask & (df['importance_level'] < 1), 'importance_level'] = 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTE3d7W8w4yp",
        "outputId": "97d912ff-d02c-4e65-9136-134cd37a6912"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before importance assignment:\n",
            "importance_level\n",
            "0    108568\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Port-based importance:\n",
            "Series([], Name: count, dtype: int64)\n",
            "\n",
            "IP-based importance:\n",
            "Series([], Name: count, dtype: int64)\n",
            "\n",
            "Application-level importance:\n",
            "Series([], Name: count, dtype: int64)\n",
            "\n",
            "Final importance distribution:\n",
            "importance_level\n",
            "0    108568\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Updated importance distribution:\n",
            "importance_level\n",
            "1    108568\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New section"
      ],
      "metadata": {
        "id": "xx02G_VaC5CS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New section"
      ],
      "metadata": {
        "id": "GF4sCiG2C7vq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCwr4naUECSB"
      },
      "outputs": [],
      "source": [
        "\n",
        "# network_trainer.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "class NetworkModelTrainer:\n",
        "    def __init__(self, df, features):\n",
        "        self.df = df\n",
        "        self.features = features\n",
        "        self.X_train = None\n",
        "        self.X_test = None\n",
        "        self.y_train = None\n",
        "        self.y_test = None\n",
        "        self.emergency_model = None\n",
        "        self.important_model = None\n",
        "\n",
        "    def prepare_data(self):\n",
        "        print(\"[INFO] Preparing data for modeling...\")\n",
        "        self.df['is_emergency'] = self.df['eisenhower_category'].apply(\n",
        "            lambda x: 1 if 'Emergency' in x else 0\n",
        "        )\n",
        "        self.df['is_important'] = self.df['eisenhower_category'].apply(\n",
        "            lambda x: 1 if 'Important' in x else 0\n",
        "        )\n",
        "\n",
        "        X = self.df[self.features]\n",
        "        y = self.df[['is_emergency', 'is_important']]\n",
        "\n",
        "\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
        "            X, y, test_size=0.3, random_state=42\n",
        "        )\n",
        "         # Print class distributions\n",
        "        print(f\"[INFO] Original 'is_emergency' distribution: {self.df['is_emergency'].value_counts()}\")\n",
        "        print(f\"[INFO] Original 'is_important' distribution: {self.df['is_important'].value_counts()}\")\n",
        "        print(f\"[INFO] Training 'is_emergency' distribution: {self.y_train['is_emergency'].value_counts()}\")\n",
        "        print(f\"[INFO] Training 'is_important' distribution: {self.y_train['is_important'].value_counts()}\")\n",
        "        print(f\"[INFO] Testing 'is_emergency' distribution: {self.y_test['is_emergency'].value_counts()}\")\n",
        "        print(f\"[INFO] Testing 'is_important' distribution: {self.y_test['is_important'].value_counts()}\")\n",
        "        print(f\"[INFO] Train shape: {self.X_train.shape}\")\n",
        "        print(f\"[INFO] Test shape: {self.X_test.shape}\")\n",
        "    \"\"\"\n",
        "    def balance_data_with_smote(self):\n",
        "        print(\"[INFO] Applying SMOTE to balance classes...\")\n",
        "        smote = SMOTE(random_state=42)\n",
        "\n",
        "        self.X_train_emergency, self.y_train_emergency = smote.fit_resample(\n",
        "            self.X_train, self.y_train['is_emergency']\n",
        "        )\n",
        "\n",
        "        self.X_train_important, self.y_train_important = smote.fit_resample(\n",
        "            self.X_train, self.y_train['is_important']\n",
        "        )\n",
        "\n",
        "        print(f\"[INFO] Emergency class distribution after SMOTE: {np.bincount(self.y_train_emergency)}\")\n",
        "        print(f\"[INFO] Important class distribution after SMOTE: {np.bincount(self.y_train_important)}\")\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    def balance_data_with_smote(self):\n",
        "        print(\"[INFO] Applying SMOTE to balance classes...\")\n",
        "        smote = SMOTE(random_state=42)\n",
        "        # Check for emergency class\n",
        "        emergency_classes = np.unique(self.y_train['is_emergency'])\n",
        "        if len(emergency_classes) > 1:\n",
        "            self.X_train_emergency, self.y_train_emergency = smote.fit_resample(\n",
        "            self.X_train, self.y_train['is_emergency']\n",
        "            )\n",
        "            print(f\"[INFO] After SMOTE - Emergency classes: {np.unique(self.y_train_emergency)}\")\n",
        "            print(f\"[INFO] After SMOTE - Emergency shape: {self.X_train_emergency.shape}\")\n",
        "        else:\n",
        "            print(f\"[WARNING] Only one class found in is_emergency: {emergency_classes}\")\n",
        "            self.X_train_emergency, self.y_train_emergency = self.X_train, self.y_train['is_emergency']\n",
        "\n",
        "        # Check for important class\n",
        "        important_classes = np.unique(self.y_train['is_important'])\n",
        "        if len(important_classes) > 1:\n",
        "          self.X_train_important, self.y_train_important = smote.fit_resample(\n",
        "            self.X_train, self.y_train['is_important']\n",
        "          )\n",
        "          print(f\"[INFO] After SMOTE - Important classes: {np.unique(self.y_train_important)}\")\n",
        "          print(f\"[INFO] After SMOTE - Important shape: {self.X_train_important.shape}\")\n",
        "        else:\n",
        "          print(f\"[WARNING] Only one class found in is_important: {important_classes}\")\n",
        "          self.X_train_important, self.y_train_important = self.X_train, self.y_train['is_important']\n",
        "        # After SMOTE\n",
        "        print(f\"[INFO] After SMOTE - 'is_emergency' distribution: {np.bincount(self.y_train_emergency)}\")\n",
        "        print(f\"[INFO] After SMOTE - 'is_important' distribution: {np.bincount(self.y_train_important)}\")\n",
        "\n",
        "    def train_emergency_model(self):\n",
        "        print(\"[INFO] Training Emergency detection model...\")\n",
        "        preprocessor = ColumnTransformer([\n",
        "            ('scale', StandardScaler(), self.features)\n",
        "        ])\n",
        "\n",
        "        self.emergency_model = Pipeline([\n",
        "            ('preprocess', preprocessor),\n",
        "            ('classifier', RandomForestClassifier(\n",
        "                n_estimators=200,\n",
        "                max_depth=10,\n",
        "                class_weight={0:1, 1:10},\n",
        "                random_state=42\n",
        "            ))\n",
        "        ])\n",
        "\n",
        "        self.emergency_model.fit(self.X_train_emergency, self.y_train_emergency)\n",
        "        print(\"[INFO] Emergency model trained successfully.\")\n",
        "\n",
        "    def train_important_model(self):\n",
        "        print(\"[INFO] Training Importance detection model...\")\n",
        "\n",
        "        preprocessor = ColumnTransformer([\n",
        "            ('scale', StandardScaler(), self.features)\n",
        "        ])\n",
        "\n",
        "        self.important_model = Pipeline([\n",
        "            ('preprocess', preprocessor),\n",
        "            ('classifier', XGBClassifier(\n",
        "                n_estimators=200,\n",
        "                learning_rate=0.1,\n",
        "                max_depth=5,\n",
        "                subsample=0.8,\n",
        "                colsample_bytree=0.8,\n",
        "                scale_pos_weight=5,\n",
        "                random_state=42\n",
        "            ))\n",
        "        ])\n",
        "\n",
        "        self.important_model.fit(self.X_train_important, self.y_train_important)\n",
        "        print(\"[INFO] Important model trained successfully.\")\n",
        "\n",
        "    def evaluate_models(self):\n",
        "        print(\"\\n=== Evaluation Results ===\")\n",
        "\n",
        "        # Emergency Model Evaluation\n",
        "        emergency_preds = self.emergency_model.predict(self.X_test)\n",
        "        print(\"\\n[Emergency Model Results]\")\n",
        "        print(classification_report(self.y_test['is_emergency'], emergency_preds))\n",
        "        print(\"Confusion Matrix:\")\n",
        "        print(confusion_matrix(self.y_test['is_emergency'], emergency_preds))\n",
        "\n",
        "        # Important Model Evaluation\n",
        "        important_preds = self.important_model.predict(self.X_test)\n",
        "        print(\"\\n[Important Model Results]\")\n",
        "        print(classification_report(self.y_test['is_important'], important_preds))\n",
        "        print(\"Confusion Matrix:\")\n",
        "        print(confusion_matrix(self.y_test['is_important'], important_preds))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CK1P3-0aIAjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJxR1YX1Ejcn"
      },
      "outputs": [],
      "source": [
        "\n",
        "# network_analyzer.py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "class NetworkAnalyzer:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def print_classification_report(self, y_true, y_pred, model_name=\"Model\"):\n",
        "        print(f\"\\n=== Classification Report: {model_name} ===\")\n",
        "        print(classification_report(y_true, y_pred))\n",
        "\n",
        "    def plot_confusion_matrix(self, y_true, y_pred, model_name=\"Model\"):\n",
        "        print(f\"[INFO] Plotting confusion matrix for {model_name}...\")\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        plt.figure(figsize=(6, 4))\n",
        "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
        "        plt.title(f\"Confusion Matrix: {model_name}\")\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('Actual')\n",
        "        plt.show()\n",
        "\n",
        "    def plot_feature_importance(self, model, feature_names, top_n=15, model_name=\"Model\"):\n",
        "        print(f\"[INFO] Plotting feature importance for {model_name}...\")\n",
        "\n",
        "        if hasattr(model.named_steps['classifier'], 'feature_importances_'):\n",
        "            importances = model.named_steps['classifier'].feature_importances_\n",
        "        else:\n",
        "            print(\"[WARNING] This model does not support feature_importances_. Skipping plot.\")\n",
        "            return\n",
        "\n",
        "        indices = np.argsort(importances)[::-1]\n",
        "        feature_names_sorted = [feature_names[i] for i in indices]\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.barplot(x=importances[indices][:top_n], y=feature_names_sorted[:top_n], palette=\"viridis\")\n",
        "        plt.title(f\"Top {top_n} Important Features ({model_name})\")\n",
        "        plt.xlabel(\"Feature Importance Score\")\n",
        "        plt.ylabel(\"Feature Names\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03k3SsZLEugN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4ab4158b-a5cc-4674-c165-c310696fb201"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Starting Network Traffic Prioritization Pipeline...\n",
            "\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            " Step 1: Preprocessing Data...\n",
            "[INFO] Loading dataset...\n",
            "[INFO] Dataset shape: (108568, 50)\n",
            "Total missing values: 0\n",
            "Columns with missing values: 0 out of 50\n",
            "\n",
            "Missing value analysis by column:\n",
            "\n",
            "Missing value analysis by row:\n",
            "Rows with at least one missing value: 0 out of 108568 (0.00%)\n",
            "\n",
            "Distribution of missing values per row:\n",
            "  - 0 missing values: 108568 rows (0.00%)\n",
            "Column Names:\n",
            "Index(['frame.time_delta', 'frame.time_relative', 'frame.len', 'ip.src',\n",
            "       'ip.dst', 'tcp.srcport', 'tcp.dstport', 'tcp.flags', 'tcp.time_delta',\n",
            "       'tcp.len', 'tcp.ack', 'tcp.connection.fin', 'tcp.connection.rst',\n",
            "       'tcp.connection.sack', 'tcp.connection.syn', 'tcp.flags.ack',\n",
            "       'tcp.flags.fin', 'tcp.flags.push', 'tcp.flags.reset', 'tcp.flags.syn',\n",
            "       'tcp.flags.urg', 'tcp.hdr_len', 'tcp.payload', 'tcp.pdu.size',\n",
            "       'tcp.window_size_value', 'tcp.checksum', 'mqtt.clientid',\n",
            "       'mqtt.clientid_len', 'mqtt.conack.flags', 'mqtt.conack.val',\n",
            "       'mqtt.conflag.passwd', 'mqtt.conflag.qos', 'mqtt.conflag.reserved',\n",
            "       'mqtt.conflag.retain', 'mqtt.conflag.willflag', 'mqtt.conflags',\n",
            "       'mqtt.dupflag', 'mqtt.hdrflags', 'mqtt.kalive', 'mqtt.len', 'mqtt.msg',\n",
            "       'mqtt.msgtype', 'mqtt.qos', 'mqtt.retain', 'mqtt.topic',\n",
            "       'mqtt.topic_len', 'mqtt.ver', 'mqtt.willmsg_len', 'ip.proto', 'ip.ttl'],\n",
            "      dtype='object')\n",
            "Number of duplciate rows: 0\n",
            "Data frame before the removal\n",
            "        frame.time_delta  frame.time_relative  frame.len        ip.src  \\\n",
            "0               0.000000             0.000000      105.0  10.5.126.141   \n",
            "1               0.000249             0.000249      105.0  10.5.126.143   \n",
            "2               0.000037             0.000286      105.0  10.5.126.145   \n",
            "3               0.000034             0.000320      105.0  10.5.126.147   \n",
            "4               0.000017             0.000337      105.0  10.5.126.141   \n",
            "...                  ...                  ...        ...           ...   \n",
            "108563          0.000008          2047.246123       80.0  10.5.150.152   \n",
            "108564          0.000058          2047.246181       72.0  10.5.150.109   \n",
            "108565          0.000011          2047.246192       72.0  10.5.150.109   \n",
            "108566          0.000010          2047.246202       72.0  10.5.150.109   \n",
            "108567          0.000008          2047.246210       72.0  10.5.150.109   \n",
            "\n",
            "              ip.dst  tcp.srcport  tcp.dstport   tcp.flags  tcp.time_delta  \\\n",
            "0        10.5.126.56        35161         1883  0x00000018        0.000000   \n",
            "1        10.5.126.56        34237         1883  0x00000018        0.000000   \n",
            "2        10.5.126.56        46623         1883  0x00000018        0.000000   \n",
            "3        10.5.126.56        45663         1883  0x00000018        0.000000   \n",
            "4        10.5.126.56        38901         1883  0x00000018        0.000000   \n",
            "...              ...          ...          ...         ...             ...   \n",
            "108563  10.5.150.109        37239         1883  0x00000018        2.998972   \n",
            "108564  10.5.150.156         1883        44939  0x00000018        0.000083   \n",
            "108565  10.5.150.157         1883        44853  0x00000018        0.000119   \n",
            "108566  10.5.150.156         1883        37361  0x00000018        0.000109   \n",
            "108567  10.5.150.157         1883        34445  0x00000018        0.000095   \n",
            "\n",
            "        tcp.len  ...  mqtt.msg  mqtt.msgtype  mqtt.qos  mqtt.retain  \\\n",
            "0            37  ...         0             1         0            0   \n",
            "1            37  ...         0             1         0            0   \n",
            "2            37  ...         0             1         0            0   \n",
            "3            37  ...         0             1         0            0   \n",
            "4            37  ...         0             1         0            0   \n",
            "...         ...  ...       ...           ...       ...          ...   \n",
            "108563       12  ...       118             3         0            0   \n",
            "108564        4  ...         0             4         0            0   \n",
            "108565        4  ...         0             4         0            0   \n",
            "108566        4  ...         0             4         0            0   \n",
            "108567        4  ...         0             4         0            0   \n",
            "\n",
            "        mqtt.topic  mqtt.topic_len  mqtt.ver  mqtt.willmsg_len  ip.proto  \\\n",
            "0                0               0         4                 0         6   \n",
            "1                0               0         4                 0         6   \n",
            "2                0               0         4                 0         6   \n",
            "3                0               0         4                 0         6   \n",
            "4                0               0         4                 0         6   \n",
            "...            ...             ...       ...               ...       ...   \n",
            "108563       CoGas               5         0                 0         6   \n",
            "108564           0               0         0                 0         6   \n",
            "108565           0               0         0                 0         6   \n",
            "108566           0               0         0                 0         6   \n",
            "108567           0               0         0                 0         6   \n",
            "\n",
            "        ip.ttl  \n",
            "0           64  \n",
            "1           64  \n",
            "2           64  \n",
            "3           64  \n",
            "4           64  \n",
            "...        ...  \n",
            "108563      64  \n",
            "108564      64  \n",
            "108565      64  \n",
            "108566      64  \n",
            "108567      64  \n",
            "\n",
            "[108568 rows x 50 columns]\n",
            "Data frame after the removal\n",
            "        frame.time_delta  frame.time_relative  frame.len        ip.src  \\\n",
            "0               0.000000             0.000000      105.0  10.5.126.141   \n",
            "1               0.000249             0.000249      105.0  10.5.126.143   \n",
            "2               0.000037             0.000286      105.0  10.5.126.145   \n",
            "3               0.000034             0.000320      105.0  10.5.126.147   \n",
            "4               0.000017             0.000337      105.0  10.5.126.141   \n",
            "...                  ...                  ...        ...           ...   \n",
            "108563          0.000008          2047.246123       80.0  10.5.150.152   \n",
            "108564          0.000058          2047.246181       72.0  10.5.150.109   \n",
            "108565          0.000011          2047.246192       72.0  10.5.150.109   \n",
            "108566          0.000010          2047.246202       72.0  10.5.150.109   \n",
            "108567          0.000008          2047.246210       72.0  10.5.150.109   \n",
            "\n",
            "              ip.dst  tcp.srcport  tcp.dstport   tcp.flags  tcp.time_delta  \\\n",
            "0        10.5.126.56        35161         1883  0x00000018        0.000000   \n",
            "1        10.5.126.56        34237         1883  0x00000018        0.000000   \n",
            "2        10.5.126.56        46623         1883  0x00000018        0.000000   \n",
            "3        10.5.126.56        45663         1883  0x00000018        0.000000   \n",
            "4        10.5.126.56        38901         1883  0x00000018        0.000000   \n",
            "...              ...          ...          ...         ...             ...   \n",
            "108563  10.5.150.109        37239         1883  0x00000018        2.998972   \n",
            "108564  10.5.150.156         1883        44939  0x00000018        0.000083   \n",
            "108565  10.5.150.157         1883        44853  0x00000018        0.000119   \n",
            "108566  10.5.150.156         1883        37361  0x00000018        0.000109   \n",
            "108567  10.5.150.157         1883        34445  0x00000018        0.000095   \n",
            "\n",
            "        tcp.len  ...  mqtt.msg  mqtt.msgtype  mqtt.qos  mqtt.retain  \\\n",
            "0            37  ...         0             1         0            0   \n",
            "1            37  ...         0             1         0            0   \n",
            "2            37  ...         0             1         0            0   \n",
            "3            37  ...         0             1         0            0   \n",
            "4            37  ...         0             1         0            0   \n",
            "...         ...  ...       ...           ...       ...          ...   \n",
            "108563       12  ...       118             3         0            0   \n",
            "108564        4  ...         0             4         0            0   \n",
            "108565        4  ...         0             4         0            0   \n",
            "108566        4  ...         0             4         0            0   \n",
            "108567        4  ...         0             4         0            0   \n",
            "\n",
            "        mqtt.topic  mqtt.topic_len  mqtt.ver  mqtt.willmsg_len  ip.proto  \\\n",
            "0                0               0         4                 0         6   \n",
            "1                0               0         4                 0         6   \n",
            "2                0               0         4                 0         6   \n",
            "3                0               0         4                 0         6   \n",
            "4                0               0         4                 0         6   \n",
            "...            ...             ...       ...               ...       ...   \n",
            "108563       CoGas               5         0                 0         6   \n",
            "108564           0               0         0                 0         6   \n",
            "108565           0               0         0                 0         6   \n",
            "108566           0               0         0                 0         6   \n",
            "108567           0               0         0                 0         6   \n",
            "\n",
            "        ip.ttl  \n",
            "0           64  \n",
            "1           64  \n",
            "2           64  \n",
            "3           64  \n",
            "4           64  \n",
            "...        ...  \n",
            "108563      64  \n",
            "108564      64  \n",
            "108565      64  \n",
            "108566      64  \n",
            "108567      64  \n",
            "\n",
            "[108568 rows x 50 columns]\n",
            "Column Names:\n",
            "Index(['frame.time_delta', 'frame.time_relative', 'frame.len', 'ip.src',\n",
            "       'ip.dst', 'tcp.srcport', 'tcp.dstport', 'tcp.flags', 'tcp.time_delta',\n",
            "       'tcp.len', 'tcp.ack', 'tcp.connection.fin', 'tcp.connection.rst',\n",
            "       'tcp.connection.sack', 'tcp.connection.syn', 'tcp.flags.ack',\n",
            "       'tcp.flags.fin', 'tcp.flags.push', 'tcp.flags.reset', 'tcp.flags.syn',\n",
            "       'tcp.flags.urg', 'tcp.hdr_len', 'tcp.payload', 'tcp.pdu.size',\n",
            "       'tcp.window_size_value', 'tcp.checksum', 'mqtt.clientid',\n",
            "       'mqtt.clientid_len', 'mqtt.conack.flags', 'mqtt.conack.val',\n",
            "       'mqtt.conflag.passwd', 'mqtt.conflag.qos', 'mqtt.conflag.reserved',\n",
            "       'mqtt.conflag.retain', 'mqtt.conflag.willflag', 'mqtt.conflags',\n",
            "       'mqtt.dupflag', 'mqtt.hdrflags', 'mqtt.kalive', 'mqtt.len', 'mqtt.msg',\n",
            "       'mqtt.msgtype', 'mqtt.qos', 'mqtt.retain', 'mqtt.topic',\n",
            "       'mqtt.topic_len', 'mqtt.ver', 'mqtt.willmsg_len', 'ip.proto', 'ip.ttl'],\n",
            "      dtype='object')\n",
            "Updated dataset shape: (108568, 50)\n",
            "[INFO] Starting feature engineering...\n",
            "[INFO] Calculating bandwidth...\n",
            "[INFO] Detecting outliers in bandwidth data...\n",
            "Bandwidth data shape: (244, 6)\n",
            "Detected 1 outliers out of 244 flows (0.41%)\n",
            "           ip.src        ip.dst  tcp.srcport  tcp.dstport  bandwidth_bps  \\\n",
            "0    10.5.126.131   10.5.126.84        41505         1883   1.574268e+07   \n",
            "1    10.5.126.131   10.5.126.84        46077         1883   1.266089e+07   \n",
            "2    10.5.126.132   10.5.126.84        34215         1883   1.577974e+07   \n",
            "3    10.5.126.132   10.5.126.84        45681         1883   8.227086e+06   \n",
            "4    10.5.126.133   10.5.126.84        44793         1883   1.695481e+07   \n",
            "..            ...           ...          ...          ...            ...   \n",
            "239  10.5.150.155  10.5.150.109        41133         1883   1.972672e+07   \n",
            "240  10.5.150.156  10.5.150.109        37361         1883   6.616093e+06   \n",
            "241  10.5.150.156  10.5.150.109        44939         1883   1.782093e+07   \n",
            "242  10.5.150.157  10.5.150.109        34445         1883   1.936095e+07   \n",
            "243  10.5.150.157  10.5.150.109        44853         1883   3.674591e+02   \n",
            "\n",
            "                                 temp_key  is_outlier   lower_bound  \\\n",
            "0     10.5.126.131_10.5.126.84_41505_1883       False -2.119925e+07   \n",
            "1     10.5.126.131_10.5.126.84_46077_1883       False -2.119925e+07   \n",
            "2     10.5.126.132_10.5.126.84_34215_1883       False -2.119925e+07   \n",
            "3     10.5.126.132_10.5.126.84_45681_1883       False -2.119925e+07   \n",
            "4     10.5.126.133_10.5.126.84_44793_1883       False -2.119925e+07   \n",
            "..                                    ...         ...           ...   \n",
            "239  10.5.150.155_10.5.150.109_41133_1883       False -2.119925e+07   \n",
            "240  10.5.150.156_10.5.150.109_37361_1883       False -2.119925e+07   \n",
            "241  10.5.150.156_10.5.150.109_44939_1883       False -2.119925e+07   \n",
            "242  10.5.150.157_10.5.150.109_34445_1883       False -2.119925e+07   \n",
            "243  10.5.150.157_10.5.150.109_44853_1883       False -2.119925e+07   \n",
            "\n",
            "      upper_bound  \n",
            "0    5.167376e+07  \n",
            "1    5.167376e+07  \n",
            "2    5.167376e+07  \n",
            "3    5.167376e+07  \n",
            "4    5.167376e+07  \n",
            "..            ...  \n",
            "239  5.167376e+07  \n",
            "240  5.167376e+07  \n",
            "241  5.167376e+07  \n",
            "242  5.167376e+07  \n",
            "243  5.167376e+07  \n",
            "\n",
            "[244 rows x 9 columns]\n",
            "[INFO] Detecting outliers in bandwidth data...\n",
            "Bandwidth data shape: (244, 6)\n",
            "Detected 1 outliers out of 244 flows (0.41%)\n",
            "[INFO] Detecting MQTT traffic...\n",
            "[INFO] Calculating flow metrics...\n",
            "[INFO] Assessing emergency and importance levels...\n",
            "[INFO] Emergency and importance levels assigned.\n",
            "[INFO] Emergency level: emergency_level\n",
            "0    108568\n",
            "Name: count, dtype: int64\n",
            "[INFO] Importance level: importance_level\n",
            "0    101342\n",
            "1      7226\n",
            "Name: count, dtype: int64\n",
            "[INFO] Flow metrics calculated.\n",
            "[INFO] Assigning Eisenhower categories...\n",
            "[INFO] Selecting final features...\n",
            "[INFO] Available features: Index(['frame.time_delta', 'frame.time_relative', 'frame.len', 'ip.src',\n",
            "       'ip.dst', 'tcp.srcport', 'tcp.dstport', 'tcp.flags', 'tcp.time_delta',\n",
            "       'tcp.len', 'tcp.ack', 'tcp.connection.fin', 'tcp.connection.rst',\n",
            "       'tcp.connection.sack', 'tcp.connection.syn', 'tcp.flags.ack',\n",
            "       'tcp.flags.fin', 'tcp.flags.push', 'tcp.flags.reset', 'tcp.flags.syn',\n",
            "       'tcp.flags.urg', 'tcp.hdr_len', 'tcp.payload', 'tcp.pdu.size',\n",
            "       'tcp.window_size_value', 'tcp.checksum', 'mqtt.clientid',\n",
            "       'mqtt.clientid_len', 'mqtt.conack.flags', 'mqtt.conack.val',\n",
            "       'mqtt.conflag.passwd', 'mqtt.conflag.qos', 'mqtt.conflag.reserved',\n",
            "       'mqtt.conflag.retain', 'mqtt.conflag.willflag', 'mqtt.conflags',\n",
            "       'mqtt.dupflag', 'mqtt.hdrflags', 'mqtt.kalive', 'mqtt.len', 'mqtt.msg',\n",
            "       'mqtt.msgtype', 'mqtt.qos', 'mqtt.retain', 'mqtt.topic',\n",
            "       'mqtt.topic_len', 'mqtt.ver', 'mqtt.willmsg_len', 'ip.proto', 'ip.ttl',\n",
            "       'bandwidth_bps', 'is_mqtt', 'flow_key_forward', 'flow_key_backward',\n",
            "       'emergency_level', 'importance_level', 'is_important_mqtt',\n",
            "       'eisenhower_category'],\n",
            "      dtype='object')\n",
            "[INFO] Selected features: ['frame.time_delta', 'frame.len', 'bandwidth_bps', 'is_mqtt']\n",
            "\n",
            " Step 2: Training Models...\n",
            "[INFO] Preparing data for modeling...\n",
            "[INFO] Original 'is_emergency' distribution: is_emergency\n",
            "1    108568\n",
            "Name: count, dtype: int64\n",
            "[INFO] Original 'is_important' distribution: is_important\n",
            "1    108568\n",
            "Name: count, dtype: int64\n",
            "[INFO] Training 'is_emergency' distribution: is_emergency\n",
            "1    75997\n",
            "Name: count, dtype: int64\n",
            "[INFO] Training 'is_important' distribution: is_important\n",
            "1    75997\n",
            "Name: count, dtype: int64\n",
            "[INFO] Testing 'is_emergency' distribution: is_emergency\n",
            "1    32571\n",
            "Name: count, dtype: int64\n",
            "[INFO] Testing 'is_important' distribution: is_important\n",
            "1    32571\n",
            "Name: count, dtype: int64\n",
            "[INFO] Train shape: (75997, 4)\n",
            "[INFO] Test shape: (32571, 4)\n",
            "[INFO] Applying SMOTE to balance classes...\n",
            "[WARNING] Only one class found in is_emergency: [1]\n",
            "[WARNING] Only one class found in is_important: [1]\n",
            "[INFO] After SMOTE - 'is_emergency' distribution: [    0 75997]\n",
            "[INFO] After SMOTE - 'is_important' distribution: [    0 75997]\n",
            "[INFO] Training Emergency detection model...\n",
            "[INFO] Emergency model trained successfully.\n",
            "[INFO] Training Importance detection model...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Invalid classes inferred from unique values of `y`.  Expected: [0], got [1]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-a52139888c60>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-a52139888c60>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbalance_data_with_smote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_emergency_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_important_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# Step 3: Evaluate Models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-6663a2ba4e38>\u001b[0m in \u001b[0;36mtrain_important_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m         ])\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimportant_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train_important\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_train_important\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[INFO] Important model trained successfully.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    660\u001b[0m                     \u001b[0mall_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m                 )\n\u001b[0;32m--> 662\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mlast_step_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"fit\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1557\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mexpected_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1558\u001b[0m             ):\n\u001b[0;32m-> 1559\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m   1560\u001b[0m                     \u001b[0;34mf\"Invalid classes inferred from unique values of `y`.  \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                     \u001b[0;34mf\"Expected: {expected_classes}, got {classes}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Invalid classes inferred from unique values of `y`.  Expected: [0], got [1]"
          ]
        }
      ],
      "source": [
        "# train_pipeline.py\n",
        "\n",
        "#from network_preprocessor import NetworkDataPreprocessor\n",
        "#%from network_trainer import NetworkModelTrainer\n",
        "#%from network_analyzer import NetworkAnalyzer\n",
        "\n",
        "def main():\n",
        "    print(\"\\n Starting Network Traffic Prioritization Pipeline...\\n\")\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    # Step 1: Preprocess the Data\n",
        "    print(\" Step 1: Preprocessing Data...\")\n",
        "    preprocessor = NetworkDataPreprocessor(filepath=\"/content/drive/MyDrive/DataSets/ICUMonitoring.csv\")\n",
        "    preprocessor.load_data()\n",
        "    preprocessor.handle_missing_values()\n",
        "    preprocessor.feature_engineering()\n",
        "    df, features = preprocessor.get_preprocessed_data()\n",
        "    print(f\"Feature names: {features}\")\n",
        "    df =preprocessor.run_complete_analysis(df, flow_metrics)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Step 2: Train Models\n",
        "    print(\"\\n Step 2: Training Models...\")\n",
        "    trainer = NetworkModelTrainer(df, features)\n",
        "    trainer.prepare_data()\n",
        "    trainer.balance_data_with_smote()\n",
        "    trainer.train_emergency_model()\n",
        "    trainer.train_important_model()\n",
        "\n",
        "    # Step 3: Evaluate Models\n",
        "    print(\"\\n Step 3: Evaluating Models...\")\n",
        "    trainer.evaluate_models()\n",
        "\n",
        "    # Step 4: Analyze Results\n",
        "    print(\"\\n Step 4: Detailed Analysis...\")\n",
        "    analyzer = NetworkAnalyzer()\n",
        "\n",
        "    # Emergency Model Evaluation\n",
        "    emergency_preds = trainer.emergency_model.predict(trainer.X_test)\n",
        "    analyzer.print_classification_report(trainer.y_test['is_emergency'], emergency_preds, model_name=\"Emergency Model\")\n",
        "    analyzer.plot_confusion_matrix(trainer.y_test['is_emergency'], emergency_preds, model_name=\"Emergency Model\")\n",
        "    analyzer.plot_feature_importance(trainer.emergency_model, features, model_name=\"Emergency Model\")\n",
        "\n",
        "    # Important Model Evaluation\n",
        "    important_preds = trainer.important_model.predict(trainer.X_test)\n",
        "    analyzer.print_classification_report(trainer.y_test['is_important'], important_preds, model_name=\"Important Model\")\n",
        "    analyzer.plot_confusion_matrix(trainer.y_test['is_important'], important_preds, model_name=\"Important Model\")\n",
        "    analyzer.plot_feature_importance(trainer.important_model, features, model_name=\"Important Model\")\n",
        "\n",
        "    print(\"\\n Pipeline completed successfully!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}